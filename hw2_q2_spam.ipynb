{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AUwbFEaMiWi"
      },
      "source": [
        "\n",
        "##Spam classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-26T18:21:06.553305Z",
          "start_time": "2022-10-26T18:21:06.548561Z"
        },
        "id": "lyLHU3Js6muG"
      },
      "source": [
        "### Step 1: Download dataset\n",
        "Download examples of spam and ham from Apache SpamAssassinâ€™s public datasets. Split the datasets into a training set and a test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-27T05:05:59.155299Z",
          "start_time": "2022-10-27T05:05:58.015019Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbs5g3YbuH59",
        "outputId": "4d8563cc-791c-48e1-df2c-385f8f101169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of training samples: 2436\n",
            "The number of test samples: 610\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "from urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import shutil\n",
        "import sklearn.utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def download_dataset():\n",
        "\n",
        "    def download_url(url, dataset_dir=\"data\"):\n",
        "\n",
        "        tar_dir = os.path.join(dataset_dir, \"tar\")\n",
        "        if not os.path.isdir(tar_dir):\n",
        "            os.makedirs(tar_dir)\n",
        "\n",
        "        filename = url.rsplit(\"/\", 1)[-1]\n",
        "        tarpath = os.path.join(tar_dir, filename)\n",
        "\n",
        "        try:\n",
        "            tarfile.open(tarpath)\n",
        "        except:\n",
        "            urlretrieve(url, tarpath)\n",
        "\n",
        "        with tarfile.open(tarpath) as tar:\n",
        "            dirname = os.path.join(dataset_dir, tar.getnames()[0])\n",
        "            if os.path.isdir(dirname):\n",
        "                shutil.rmtree(dirname)\n",
        "            tar.extractall(path=dataset_dir)\n",
        "\n",
        "            cmds_path = os.path.join(dirname, \"cmds\")\n",
        "            if os.path.isfile(cmds_path):\n",
        "                os.remove(cmds_path)\n",
        "\n",
        "        return dirname\n",
        "\n",
        "    def load_dataset(dirpath):\n",
        "        files = []\n",
        "        filepaths = glob.glob(dirpath + \"/*\")\n",
        "        for path in filepaths:\n",
        "            with open(path, \"rb\") as f:\n",
        "                byte_content = f.read()\n",
        "                str_content = byte_content.decode(\"utf-8\", errors=\"ignore\")\n",
        "                files.append(str_content)\n",
        "        return files\n",
        "\n",
        "    spam_url = \"https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2\"\n",
        "    easy_ham_url = \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\"\n",
        "    hard_ham_dir = \"https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\"\n",
        "\n",
        "    spam = load_dataset(download_url(spam_url))\n",
        "    easy_ham = load_dataset(download_url(easy_ham_url))\n",
        "    hard_ham = load_dataset(download_url(hard_ham_dir))\n",
        "\n",
        "    X = spam + easy_ham + hard_ham\n",
        "    y = np.concatenate((\n",
        "        np.ones(len(spam)),\n",
        "        np.zeros(len(easy_ham) + len(hard_ham)),\n",
        "    ))\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# Download dataset.\n",
        "X, y = download_dataset()\n",
        "\n",
        "# Split dataset into training and testing sets.\n",
        "X, y = sklearn.utils.shuffle(X, y, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    stratify=y,\n",
        "                                                    random_state=42)\n",
        "\n",
        "print(f\"The number of training samples: {len(X_train)}\")\n",
        "print(f\"The number of test samples: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4RYzHc0vsyi"
      },
      "source": [
        "### Step 2: Feature extraction (5 points)\n",
        "\n",
        "Next, we are going to do some data cleaning and feature extraction.\n",
        "\n",
        "1. Some data cleaning functions have been provided to you. You'll need to implement `lower_letters()`, `convert_num_to_word()`, and `remove_punctuation()`. These functions will convert email to lowercase, replace all numbers with \"NUM\", and remove punctuation.\n",
        "2. Convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector that indicates the presence or absence of each possible word. For example, if all emails only ever contain four words, \"Hello,\" \"how,\" \"are,\" \"you,\" then the email \"Hello you Hello Hello you\" would be converted into a vector [1, 0, 0, 1] (meaning [\"Hello\" is present, \"how\" is absent, \"are\" is absent, \"you\" is present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of each word. You may check sklearn's `CountVectorizer` class for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-27T05:05:59.258405Z",
          "start_time": "2022-10-27T05:05:59.252825Z"
        },
        "id": "fegetO2HwHBo"
      },
      "outputs": [],
      "source": [
        "import enum\n",
        "import re\n",
        "import string\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "class EmailCleaner(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self,\n",
        "                 no_header=True,\n",
        "                 to_lowercase=True,\n",
        "                 url_to_word=True,\n",
        "                 num_to_word=True,\n",
        "                 remove_punc=True):\n",
        "        self.no_header = no_header\n",
        "        self.to_lowercase = to_lowercase\n",
        "        self.url_to_word = url_to_word\n",
        "        self.num_to_word = num_to_word\n",
        "        self.remove_punc = remove_punc\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_cleaned = []\n",
        "        for email in X:\n",
        "            if self.no_header:\n",
        "                email = EmailCleaner.remove_header(email)\n",
        "            if self.to_lowercase:\n",
        "                email = EmailCleaner.lower_letters(email)\n",
        "\n",
        "            email_words = email.split()\n",
        "            if self.url_to_word:\n",
        "                email_words = EmailCleaner.convert_url_to_word(email_words)\n",
        "            if self.num_to_word:\n",
        "                email_words = EmailCleaner.convert_num_to_word(email_words)\n",
        "            email = \" \".join(email_words)\n",
        "            if self.remove_punc:\n",
        "                email = EmailCleaner.remove_punctuation(email)\n",
        "            X_cleaned.append(email)\n",
        "        return X_cleaned\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_header(email):\n",
        "        return email[email.index(\"\\n\\n\"):]\n",
        "\n",
        "    @staticmethod\n",
        "    def is_url(s):\n",
        "        url = re.match(\n",
        "            \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|\"\n",
        "            \"[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", s)\n",
        "        return url is not None\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_url_to_word(words):\n",
        "        for i, word in enumerate(words):\n",
        "            if EmailCleaner.is_url(word):\n",
        "              words[i] = \"URL\"\n",
        "        return words\n",
        "\n",
        "    @staticmethod\n",
        "    def lower_letters(email):\n",
        "      return email.lower()\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_num_to_word(words):\n",
        "      for i, word in enumerate(words):\n",
        "        if type(word) == int:\n",
        "          words[i] = \"NUM\"\n",
        "\n",
        "        elif word.isnumeric():\n",
        "          words[i] = \"NUM\"\n",
        "      return words\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_punctuation(email):\n",
        "      return email.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_extraInfo(email):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "class countVectorizer:\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    vectorizer = CountVectorizer()\n",
        "    bag = vectorizer.fit_transform(X)\n",
        "    return bag\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "gowZ8YDUqsVM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-27T05:06:00.474628Z",
          "start_time": "2022-10-27T05:06:00.471273Z"
        },
        "id": "yMyea7Ne6muK"
      },
      "outputs": [],
      "source": [
        "# Here are some unit tests to check your code.\n",
        "# Your code should at least pass the following tests.\n",
        "# Feel free to add more tests if you\"d like.\n",
        "\n",
        "# Check lower_letters().\n",
        "src_string = \"Message-Id: <LISTMANAGERSQL-25343\"\n",
        "dst_string = \"message-id: <listmanagersql-25343\"\n",
        "assert EmailCleaner.lower_letters(src_string) == dst_string\n",
        "\n",
        "# Check convert_num_to_word().\n",
        "src_string = \"Date: Wed, 10 Jul 2002\"\n",
        "src_words = src_string.split()\n",
        "dst_words = [\"Date:\", \"Wed,\", \"NUM\", \"Jul\", \"NUM\"]\n",
        "assert EmailCleaner.convert_num_to_word(src_words) == dst_words\n",
        "\n",
        "# Check remove_punctuation().\n",
        "src_string = \"superstars -- you'll find investing more fun...\"\n",
        "dst_string = \"superstars  youll find investing more fun\"\n",
        "assert EmailCleaner.remove_punctuation(src_string) == dst_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-27T05:06:02.761239Z",
          "start_time": "2022-10-27T05:06:01.254913Z"
        },
        "id": "g_X-4Cli6muL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56ce0f4-c590-45bf-d3aa-b29667db0159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2436, 108845)\n",
            "(610, 108845)\n"
          ]
        }
      ],
      "source": [
        "# Step 1 of pipeline: data cleaning.\n",
        "email_cleaner = EmailCleaner()\n",
        "\n",
        "# Step 2 of pipeline: CountVectorizer.\n",
        "count_vectorizer = countVectorizer()\n",
        "\n",
        "# Build pipeline.\n",
        "prepare_pipeline = Pipeline([\n",
        "    (\"email_cleaner\", email_cleaner),\n",
        "    (\"count_vectorizer\", count_vectorizer),\n",
        "])\n",
        "\n",
        "# Run preprocessing.\n",
        "X_all = X_train + X_test\n",
        "prepare_pipeline.fit(X_all)\n",
        "X_all = prepare_pipeline.transform(X_all)\n",
        "num_train = len(X_train)\n",
        "X_train = X_all[:num_train]\n",
        "X_test = X_all[num_train:]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZYuW1qnvth8"
      },
      "source": [
        "### Step 3: Train a spam classifier (5 points)\n",
        "\n",
        "Next, let's build a spam classifier, and train your classifier with the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-27T05:06:25.443785Z",
          "start_time": "2022-10-27T05:06:25.420443Z"
        },
        "id": "qyq7_7px6muM"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf_one = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "clf_one.fit(X_train, y_train)\n",
        "y_prediction = clf_one.predict(X_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xc_bkNi6muM"
      },
      "source": [
        "### Step 4: Eval your classifier\n",
        "\n",
        "Test your classifier with the test set and print the precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-27T05:06:26.371331Z",
          "start_time": "2022-10-27T05:06:26.364811Z"
        },
        "id": "zLjnVdg76muN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc32d997-cd81-4b94-cba6-e1ad5b8b4e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.930\n",
            "Recall: 0.904\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "print('Precision: %.3f' %precision_score(y_true=y_test, y_pred=y_prediction))\n",
        "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTbxtjvHohqJ"
      },
      "source": [
        "### Step 5: Ensemble of classifiers (5 points)\n",
        "\n",
        "1. Implement 4 new classifiers (in total you have 5 claassifiers now).\n",
        "2. Use hard or soft voting to ensemble thoses classifiers.\n",
        "3. Train your ensemble model on the training set. Report training/testing precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-27T05:06:29.164535Z",
          "start_time": "2022-10-27T05:06:28.530051Z"
        },
        "id": "QwS_cYldop6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b242ba-c9d3-47de-c63c-adde376f28c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.989\n",
            "Recall: 0.950\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
        "\n",
        "\n",
        "clf_two = LogisticRegression(C=10, random_state=0, max_iter=1000)\n",
        "clf_two.fit(X_train, y_train)\n",
        " \n",
        "clf_three =  KNeighborsClassifier(p=2)\n",
        "clf_three.fit(X_train, y_train)\n",
        "\n",
        "clf_four = SVC(probability=True)\n",
        "clf_four.fit(X_train, y_train)\n",
        "\n",
        "clf_five =  RandomForestClassifier(n_jobs=-1)\n",
        "clf_five.fit(X_train, y_train)\n",
        "\n",
        "estimators = [('dt', clf_one), ('lr', clf_two), ('knn', clf_three), ('svm', clf_four), ('prc', clf_five)]\n",
        "ensemble_clf = VotingClassifier(estimators=estimators, voting=\"soft\")\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "y_pred = ensemble_clf.predict(X_test)\n",
        "\n",
        "print('Precision: %.3f' %precision_score(y_true=y_test, y_pred=y_pred))\n",
        "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "6b2aedd612e0cd6f0ee559b9853149124d77e4d39b549e65a04cac10fc7055af"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}